{"cells":[{"cell_type":"markdown","metadata":{"id":"zvJmNUVfeen6"},"source":["# ML Jour 2 – Cas Olist"]},{"cell_type":"markdown","metadata":{"id":"k52-A9Gmeen_"},"source":["Pour les 2 premiers jours de ML, nous utiliserons à nouveau les données Olist. Le dataset est disponible sur Notion dans ML Day 1, sur le serveur DBeaver et sur [Kaggle](https://www.kaggle.com/olistbr/brazilian-ecommerce).\n","\n","Le but de ce use case est de monter en compétence sur les différentes techniques et modèles que nous verrons en cours, et d'apprendre à utiliser Dataiku pour mettre en place un workflow de machine learning."]},{"cell_type":"markdown","metadata":{"id":"XJmMyCYseen_"},"source":["**Objectif** : Prédire la customer satisfaction après réception d'une commande. Notre objectif sera donc de prédire la note de la review associée à une commande. Notre variable cible `y` est donc la variable *review_score*."]},{"cell_type":"markdown","metadata":{"id":"UFtdtvn5eeoA"},"source":["**Jour 1** : Pour commencer, on va se concentrer sur l'EDA et sur la phase de pré-processing des données, de manère à produire un dataset propre utilisant un subset de variables du dataset.\n","\n","**Jour 2** : Ensuite, on utilisera ce dataset pour effectuer une régression linéaire, puis une régression logistique.\n","\n","**Jour 3** : Enfin, nous mettrons à profit ce que nous avons vu sur la plateforme de ML no-code Dataiku, en répliquant le pipeline entier du pré-processing jusqu'au modèle de régression linéaire.\n","\n","Pour les 3 jours, l'objectif est de **comprendre le fonctionnement d'un pipeline de ML sur Dataiku et sur Python**. Le plus important n'est pas de connaître par cœur le code mais de comprendre qu'un pipeline de ML est très standardisé quel que soit le type de problème (régression, classification)."]},{"cell_type":"markdown","metadata":{"id":"QgFYkAwheeoB"},"source":["## Data loading"]},{"cell_type":"markdown","metadata":{"id":"wXATwzfmeeoB"},"source":["Nous allons utiliser le dataset nettoyé que nous avons construit hier."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-04-28T23:02:08.618686Z","start_time":"2021-04-28T23:02:07.349682Z"},"id":"JiP1Aj9beeoC"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","pd.set_option(\"max_columns\", 100)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-01-31T18:20:37.279958Z","start_time":"2021-01-31T18:20:35.742125Z"},"id":"pPYTOCS2eeoD"},"outputs":[],"source":["df_olist = pd.read_csv(\"olist_dataset/olist_cleaned.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-01-31T18:20:37.954922Z","start_time":"2021-01-31T18:20:37.282605Z"},"scrolled":false,"id":"03Pf9XiyeeoE"},"outputs":[],"source":["df_olist.head(3)"]},{"cell_type":"markdown","metadata":{"id":"d5XRhsNYeeoF"},"source":["## Etape 4 : normalisation et encoding des features catégorielles\n"]},{"cell_type":"markdown","metadata":{"id":"mGUgvxZreeoF"},"source":["### Normalisation\n","\n","Normalisez les variables numériques, à l'exception de la variable cible, à l'aide du `StandardScaler`.\n","\n","Recréez un dataframe final, qui contient toutes les variables numériques normalisées et la variable cible."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bXB6lKwteeoG"},"outputs":[],"source":["# import and instantiate StandardScaler object\n","from sklearn.preprocessing import StandardScaler\n","\n","standard_scal = StandardScaler()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aiGhkT_LeeoG"},"outputs":[],"source":["\n","# select only numeric variables\n","\n","num_cols = df_olist.select_dtypes(include=\"number\").drop(columns='review_score').columns\n","df_olist_num = df_olist.copy()\n","df_olist_num = df_olist_num[num_cols]\n","df_olist[num_cols] = standard_scal.fit_transform(df_olist_num.values)\n","\n","df_olist.head(3)"]},{"cell_type":"markdown","metadata":{"id":"RjFmjMjBeeoG"},"source":["### Encoder les variables catégorielles\n","Isolez les 3 variables catégorielles \"payment_type\", \"order_status\", \"product_category_name\" et affichez des `value_counts`.\n","\n","Utilisez la méthode du one-hot encoding sur ces 3 variables. Pour les catégories de produits, vous pouvez regrouper les petites catégories sous le nom \"other\".\n","\n","Il est également possible ici d'encoder les informations géographiques. Créez 2 nouvelles colonnes, qui montrent respectivement si le vendeur et l'acheteur sont dans le même état et la même ville."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ej-1Py7eeoH"},"outputs":[],"source":["categorical_variables = [\"payment_type\", \"order_status\", \"product_category_name\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ab0jtFHieeoH"},"outputs":[],"source":["df_olist.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aqwq9WJkeeoH"},"outputs":[],"source":["df_olist[\"product_category_name\"].nunique()"]},{"cell_type":"markdown","metadata":{"id":"0tgkK5jzeeoH"},"source":["Dans notre cas, il existe 73 modalités différentes pour le nom de catégorie. C'est très élevé et cela risque de perturber le modèle de ML ! Nous allons donc rassembler les modalités en ne gardant que les 20 premières (c'est un choix arbitraire) tout en rassemblant les autres dans une catégorie \"autres\"."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DUWIva1reeoI"},"outputs":[],"source":["other_categories = df_olist[\"product_category_name\"].value_counts().index[20:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wmFixf7DeeoI"},"outputs":[],"source":["def category_group(x):\n","    if x in other_categories:\n","        return \"other\"\n","    else:\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UOSM2L7jeeoI"},"outputs":[],"source":["df_olist[\"product_category_name\"] = df_olist[\"product_category_name\"].apply(lambda x: category_group(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3nKnzxGCeeoI"},"outputs":[],"source":["# vérification de la nouvelle catégorie \"other\"\n","df_olist[\"product_category_name\"].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ep4T1fxNeeoJ"},"outputs":[],"source":["# encoding des catégories de \"product_category_name\"\n","ohe_product_categ = pd.get_dummies(df_olist[\"product_category_name\"], prefix=\"categ\")\n","df_olist = pd.concat([df_olist, ohe_product_categ], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"El99oPGLeeoJ"},"outputs":[],"source":["# encoding des catégories de \"order_status\"\n","ohe_order_status = pd.get_dummies(df_olist[\"order_status\"], prefix=\"order_status\")\n","df_olist = pd.concat([df_olist, ohe_order_status], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0lZehprieeoJ"},"outputs":[],"source":["# encoding des catégories de \"payment_type\"\n","ohe_payment_type = pd.get_dummies(df_olist[\"payment_type\"], prefix=\"payment_type\")\n","df_olist = pd.concat([df_olist, ohe_payment_type], axis=1)\n","df_olist"]},{"cell_type":"markdown","metadata":{"id":"AajJPuFleeoJ"},"source":["Cependant, il est cette fois nécessaire de supprimer toutes les colonnes de date et les colonnes catégoriques qui n'ont pas été encodées, car elles ne sont pas interprétables par le modèle, qui va crasher."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"py_RkzcreeoJ"},"outputs":[],"source":["cols_to_drop = ['product_category_name', 'order_status', 'payment_type'] "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZOCdgSfQeeoK"},"outputs":[],"source":["df_olist = df_olist.drop(columns=cols_to_drop)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3mtqJfbCeeoK"},"outputs":[],"source":["df_olist = df_olist.dropna()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MbqDjONdeeoK"},"outputs":[],"source":["df_olist.head(3)"]},{"cell_type":"markdown","metadata":{"id":"-KNNbphSeeoK"},"source":["## Etape 4 - Fit"]},{"cell_type":"markdown","metadata":{"id":"whe9bSqieeoK"},"source":["### Création des 4 datasets utilisés par le modèle"]},{"cell_type":"markdown","metadata":{"id":"cwI4lFdueeoK"},"source":["1) Séparer les features (`X`) de la target (`y`).\n","\n","2) Réaliser un train-test split qui va permettre de construire nos 4 datasets de fin : X train, X test, y train et y test."]},{"cell_type":"markdown","metadata":{"id":"i84eba58eeoK"},"source":["Réaliser un train-test split avec 75% des données en train. Ajouter `random_state=42` en argument de la fonction `train_test_split` afin que l'on ait des résultats comparables."]},{"cell_type":"markdown","metadata":{"id":"IvXbth03eeoK"},"source":["<blockquote>Comment un train-test split fonctionne-t-il ? A partir d'un dataset X (nos variables de prédiction) et d'une series y (notre variable à prédire), on va prendre un échantillon de lignes de X et de y (avec les mêmes index) pour faire le train dataset, et le reste sera le test dataset. Par exemple, si l'on prend un dataset de 10 lignes avec 80% des lignes dans le train, on choisit 8 index au hasard (ex: 0, 1, 2, 4, 5, 7, 8, 9) du dataset. Ensuite, les lignes correspondantes de X forment X train tandis que les lignes restantes forment X test. De même, les lignes correspondantes de y forment y train tandis que les lignes restantes forment y test.\n","</blockquote>\n","\n","![Train test split](train_test_split_2.png)\n","\n","3 remarques :\n","- Il faut absolument conserver les mêmes index entre X et y, sinon les features qui servent à prédire s'entraîneront sur les mauvais y !\n","- Il est possible de faire le découpage des 4 datasets en faisant d'abord le train test split puis en séparant ensuite le train et le test dataset entre X et y. Cela ne change rien, les 2 manières de faire étant identiques tant qu'elles amènent bien à X train, y train, X test et y test.\n","- En pratique, nous utiliserons la fonction train_test_split de scikit-learn qui s'occupe automatiquement de splitter X et y."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-05-02T10:07:12.566830Z","start_time":"2021-05-02T10:07:12.493438Z"},"id":"NKNfWZvyeeoL"},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-04-29T22:44:15.999784Z","start_time":"2021-04-29T22:44:15.953372Z"},"id":"NMEUmJcueeoL"},"outputs":[],"source":["# on exclut bien sûr la colonne review_score des features\n","\n","X = df_olist.drop(\"review_score\", axis=1)\n","y = df_olist[\"review_score\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-05-02T10:07:13.673154Z","start_time":"2021-05-02T10:07:13.484627Z"},"id":"MYbPeeeaeeoL"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vg4YicjXeeoL"},"outputs":[],"source":["X_train.describe()"]},{"cell_type":"markdown","metadata":{"id":"fSnRuQE9eeoM"},"source":["Nous pouvons maintenant construire le modèle de régression linéaire. L'objet s'importe facilement depuis la librairie scikit-learn. Nous allons également importer la métrique de r2 qui servira à évaluer le modèle."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-04-29T22:44:50.234062Z","start_time":"2021-04-29T22:44:50.222861Z"},"id":"O1XVmXGWeeoM"},"outputs":[],"source":["# on importe les sous-modules de sklearn dont on a besoin\n","\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import r2_score"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-04-29T22:44:50.783473Z","start_time":"2021-04-29T22:44:50.651334Z"},"scrolled":true,"id":"Ts04hRV7eeoM"},"outputs":[],"source":["# on instancie un objet LinearRegression, puis on le \"fit\"\n","lin_reg = LinearRegression()\n","lin_reg.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-04-29T22:44:52.581463Z","start_time":"2021-04-29T22:44:52.568359Z"},"id":"kNh4j1jmeeoM"},"outputs":[],"source":["# on utilise cet objet pour prédire les valeurs sur le dataset\n","y_pred = lin_reg.predict(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-04-29T22:44:53.791612Z","start_time":"2021-04-29T22:44:53.782405Z"},"scrolled":true,"id":"X3xNPG4neeoM"},"outputs":[],"source":["# on évalue les prédictions avec le R2\n","score = r2_score(y, y_pred)\n","print(f\"Le R2 est de {score:.1%}!\")"]},{"cell_type":"markdown","metadata":{"id":"aDVMVAAxeeoN"},"source":["### Etape 5 - Evaluation"]},{"cell_type":"markdown","metadata":{"id":"Va7W36n-eeoN"},"source":["Nous allons analyser le résultat de la régression linéaire :\n","\n","- Analyse des valeurs des coefficients: afficher un graphique de feature importance en utilisant la fonction vue dans le live-coding. Ces résultats vous semblent-ils cohérents?\n","\n","- Significativité des coefficients (t-statistic and p-value), intervalle de confiance des coefficients: informations disponibles dans le résumé `statsmodel` si vous utilisez ce package."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-04-29T14:40:31.839768Z","start_time":"2021-04-29T14:40:31.830668Z"},"scrolled":true,"id":"pjWQVWnAeeoN"},"outputs":[],"source":["# on affiche les coefficients avec les features associées\n","\n","for feat, coef in zip(X.columns, lin_reg.coef_):\n","    print(feat, f\"{coef:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-04-29T15:55:18.696777Z","start_time":"2021-04-29T15:55:18.646816Z"},"id":"8scbbKSSeeoN"},"outputs":[],"source":["def feature_importance(model, X_train):\n","    \"\"\"\n","    Plots a feature importance graph for regressions (linear, logistic, regularizations...)\n","    or random forest models.\n","    \n","    Args:\n","        model: trained model\n","        X_train: the training dataframe, to extract variable names\n","    \"\"\"\n","    \n","    try:\n","        try:\n","            importance = model.coef_[0]\n","            test_error = importance[0]\n","        except:\n","            importance = model.coef_\n","\n","        importances = []\n","        for i, v in enumerate(importance):\n","            importances.append((X_train.columns[i], v))\n","        importances.sort(key=lambda tup: abs(tup[1]), reverse=True)\n","    \n","        feature_names = [x[0] for x in importances]\n","        importances = [x[1] for x in importances]\n","        \n","    \n","    except:\n","        try:\n","            ordering = np.argsort(model.feature_importances_)[::-1]#[:50]\n","            importances = model.feature_importances_[ordering]\n","\n","            X_columns = X_train.columns\n","            feature_names = X_columns[ordering]\n","        \n","        except:\n","            print('The function can only plot feature importance for regression or RF models.')\n","        \n","    ticks = np.arange(len(importances))\n","    fig, ax = plt.subplots(figsize=(16,5))\n","    ax = sns.barplot(y=importances, x=ticks, palette=sns.diverging_palette(150, 10, center=\"dark\", n=len(importances)))\n","    plt.xticks(ticks, feature_names, rotation=90)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2021-04-29T15:55:34.356691Z","start_time":"2021-04-29T15:55:33.512040Z"},"scrolled":true,"id":"GODbk2bLeeoN"},"outputs":[],"source":["feature_importance(lin_reg, X)"]},{"cell_type":"markdown","metadata":{"id":"JGM_cPe4eeoO"},"source":["# Régression logistique"]},{"cell_type":"markdown","metadata":{"id":"E4RjJTjseeoO"},"source":["Pour vous montrer que les pipelines suivent la même structure même dans le cas de deux problèmes différents, nous allons maintenant construire une régression logistique.\n","\n","Dans ce cas précis, comme la valeur à prédire est discrète, nous pouvons au choix effectuer une régression ou une classification. Si la valeur à prédire avait été continue comme un prix par exemple, nous n'aurions pu utiliser que la régression.\n","\n","Essayons donc de refaire le même procédé avec une régression logistique. Le préprocessing est identique, importons donc juste le nouvel objet de régression logistique en conservant les mêmes datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iJAiSPnreeoO"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VZZSephZeeoO"},"outputs":[],"source":["print(y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eL2VnGqleeoO"},"outputs":[],"source":["# on instancie un objet LinearRegression, puis on le \"fit\"\n","log_reg = LogisticRegression()\n","log_reg.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mS6j9wDeeeoO"},"outputs":[],"source":["# on utilise cet objet pour prédire les valeurs sur le dataset\n","y_pred = lin_reg.predict(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AxHEb8N2eeoP"},"outputs":[],"source":["# on évalue les prédictions avec le R2\n","score = r2_score(y, y_pred)\n","print(f\"Le R2 est de {score:.1%}!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"upRYX6pleeoP"},"outputs":[],"source":["feature_importance(lin_reg, X)"]}],"metadata":{"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true},"colab":{"name":"ML – Cas Olist J2 – Correction.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}